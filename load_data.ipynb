{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8TvKTPX5JqEt9lyZiYb9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NINE9-9-9/Flood-Detection/blob/main/load_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/spaceml-org/ml4floods#egg=ml4floods"
      ],
      "metadata": {
        "id": "BcoS80LEXRVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional, List, Callable, Dict\n",
        "from torch.utils.data import DataLoader\n",
        "from ml4floods.data.worldfloods.dataset import WorldFloodsDatasetTiled, WorldFloodsDataset\n",
        "from ml4floods.data.worldfloods.configs import BANDS_S2, BANDS_L8\n",
        "import pytorch_lightning as pl\n",
        "from ml4floods.preprocess.tiling import WindowSize\n",
        "from ml4floods.preprocess.utils import get_list_of_window_slices\n",
        "\n",
        "\n",
        "class WorldFloodsDataModule(pl.LightningDataModule):\n",
        "    \"\"\"A prepackaged WorldFloods Pytorch-Lightning data module\n",
        "    This initializes a module given a set a directory with a subdirectory\n",
        "    for the training and testing data (\"image_folder\" and \"target_folder\").\n",
        "    Then we can search through the directory and load the images found. It\n",
        "    creates the train, val and test datasets which then can be used to initialize\n",
        "    the dataloaders. This is pytorch lightning compatible which can be used with\n",
        "    the training fit framework.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): the input folder sub_directory\n",
        "        target_folder (str): the target folder sub directory\n",
        "        train_transformations (Callable): the transformations used within the\n",
        "            training data module\n",
        "        test_transformations (Callable): the transformations used within the\n",
        "            testing data module\n",
        "        window_size (Tuple[int,int]): the window size used to tile the images\n",
        "            for training\n",
        "        batch_size (int): the batchsize used for the dataloader\n",
        "        bands (List(int)): the bands to be selected from the images\n",
        "\n",
        "    Attributes:\n",
        "        train_transform (Callable): the transformations used within the\n",
        "            training data module\n",
        "        test_transform (Callable): the transformations used within the\n",
        "            testing data module\n",
        "        bands (List(int)): the bands to be selected from the images\n",
        "        image_prefix (str): the input folder sub_directory\n",
        "        gt_prefix (str): the target folder sub directory\n",
        "        window_size (Tuple[int,int]): the window size used to tile the images\n",
        "            for training\n",
        "        filter_windows (Callable): function to filter the training tiles by\n",
        "            number of invalid and cloud pixels\n",
        "        filenames_train_test (Dict): path to images and ground truth for\n",
        "            the training, validation and test splits\n",
        "\n",
        "    Example:\n",
        "        >>> from ml4floods.data.worldfloods.lightning import WorldFloodsDataModule\n",
        "        >>> wf_dm = WorldFloodsDataModule()\n",
        "        >>> wf_dm.prepare_data()\n",
        "        >>> wf_dm.setup()\n",
        "        >>> train_dl = wf_dm.train_dataloader()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        filenames_train_test: Dict,\n",
        "        input_folder: str = \"S2\",\n",
        "        target_folder: str = \"gt\",\n",
        "        train_transformations: Optional[Callable] = None,\n",
        "        test_transformations: Optional[Callable] = None,\n",
        "        add_mndwi_input: bool = False,\n",
        "        window_size: Tuple[int, int] = (64, 64),\n",
        "        batch_size: int = 32,\n",
        "        bands: List[int] = [1, 2, 3],\n",
        "        num_workers:int = 4,\n",
        "        num_workers_val:int = 0,\n",
        "        num_workers_test: int = 0,\n",
        "        filter_windows:Callable = None,\n",
        "        lock_read: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_transform = train_transformations\n",
        "        self.test_transform = test_transformations\n",
        "        self.num_workers = num_workers\n",
        "        self.num_workers_test = num_workers_test\n",
        "        self.num_workers_val = num_workers_val\n",
        "        self.lock_read = lock_read\n",
        "\n",
        "        # self.dims is returned when you call dm.size()\n",
        "        # Setting default dims here because we know them.\n",
        "        # Could optionally be assigned dynamically in dm.setup()\n",
        "        self.bands = bands\n",
        "        self.add_mndwi_input = add_mndwi_input\n",
        "        self.batch_size = batch_size\n",
        "        # Prefixes\n",
        "        self.image_prefix = input_folder\n",
        "        self.gt_prefix = target_folder\n",
        "        self.filter_windows = filter_windows\n",
        "        self.window_size = WindowSize(height=window_size[0], width=window_size[1])\n",
        "        self.filenames_train_test = filenames_train_test\n",
        "\n",
        "        files = {}\n",
        "        splits = [\"train\", \"test\", \"val\"]\n",
        "\n",
        "        # loop through the naming splits\n",
        "        for isplit in splits:\n",
        "                # TODO we might could use the train_test_split dict directly to avoid using image_prefix and gt_prefix\n",
        "                files[isplit] = self.filenames_train_test[isplit][self.image_prefix]\n",
        "\n",
        "        # save filenames\n",
        "        self.train_files = files[\"train\"]\n",
        "        self.val_files = files[\"val\"]\n",
        "        self.test_files = files[\"test\"]\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Does Nothing for now. Here for compatibility.\"\"\"\n",
        "        # TODO: here we can check for correspondence between the files\n",
        "        pass\n",
        "\n",
        "    def get_mndwi_indices(self, bands):\n",
        "        band_names_current_image = [BANDS_S2[iband] for iband in bands]\n",
        "        mndwi_indexes_current_image = [band_names_current_image.index(b) for b in [\"B3\", \"B11\"]]\n",
        "        return mndwi_indexes_current_image\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"This creates the PyTorch dataset given the preconfigured\n",
        "        file paths.\n",
        "        \"\"\"\n",
        "\n",
        "        self.train_dataset = WorldFloodsDatasetTiled(\n",
        "            list_of_windows=get_list_of_window_slices(self.train_files, window_size=self.window_size),\n",
        "            image_prefix=self.image_prefix,\n",
        "            gt_prefix=self.gt_prefix,\n",
        "            bands=self.bands,\n",
        "            mndwi_indices = self.get_mndwi_indices(self.bands) if self.add_mndwi_input else None,\n",
        "            transforms=self.train_transform,\n",
        "            lock_read=self.lock_read\n",
        "        )\n",
        "        if self.filter_windows is not None:\n",
        "            self.train_dataset.list_of_windows = self.filter_windows(self.train_dataset)\n",
        "\n",
        "        self.val_dataset = WorldFloodsDatasetTiled(\n",
        "            list_of_windows=get_list_of_window_slices(\n",
        "                self.val_files, window_size=self.window_size\n",
        "            ),\n",
        "            image_prefix=self.image_prefix,\n",
        "            gt_prefix=self.gt_prefix,\n",
        "            bands=self.bands,\n",
        "            mndwi_indices = self.get_mndwi_indices(self.bands) if self.add_mndwi_input else None,\n",
        "            transforms=self.test_transform,\n",
        "            lock_read=self.lock_read\n",
        "        )\n",
        "\n",
        "        self.test_dataset = WorldFloodsDataset(\n",
        "            image_files=self.test_files,\n",
        "            image_prefix=self.image_prefix,\n",
        "            gt_prefix=self.gt_prefix,\n",
        "            bands=self.bands,\n",
        "            mndwi_indices = self.get_mndwi_indices(self.bands) if self.add_mndwi_input else None,\n",
        "            transforms=self.test_transform,\n",
        "            lock_read=self.lock_read\n",
        "        )\n",
        "\n",
        "        self.test_dataset2 = WorldFloodsDatasetTiled(\n",
        "            list_of_windows=get_list_of_window_slices(\n",
        "                self.val_files, window_size=self.window_size\n",
        "            ),\n",
        "            image_prefix=self.image_prefix,\n",
        "            gt_prefix=self.gt_prefix,\n",
        "            bands=self.bands,\n",
        "            mndwi_indices = self.get_mndwi_indices(self.bands) if self.add_mndwi_input else None,\n",
        "            transforms=self.test_transform,\n",
        "            lock_read=self.lock_read\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Initializes and returns the training dataloader\"\"\"\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self, num_workers=None):\n",
        "        \"\"\"Initializes and returns the validation dataloader\"\"\"\n",
        "        num_workers = num_workers or self.num_workers_val\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
        "                          num_workers=num_workers, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self, num_workers=None):\n",
        "        \"\"\"Initializes and returns the test dataloader\"\"\"\n",
        "        num_workers = num_workers or self.num_workers_test\n",
        "        return DataLoader(self.test_dataset, batch_size=1,\n",
        "                          num_workers=num_workers, shuffle=False)\n",
        "    def test_dataloader2(self, num_workers=None):\n",
        "        \"\"\"Initializes and returns the validation dataloader\"\"\"\n",
        "        num_workers = num_workers or self.num_workers_val\n",
        "        return DataLoader(self.test_dataset2, batch_size=self.batch_size,\n",
        "                          num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "4Eklf-qNxpoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaUeZImWo04"
      },
      "outputs": [],
      "source": [
        "from ml4floods.preprocess.worldfloods import normalize as wf_normalization\n",
        "import ml4floods.preprocess.transformations as transformations\n",
        "from ml4floods.preprocess.tiling import WindowSlices, load_windows, save_windows\n",
        "from glob import glob\n",
        "from ml4floods.data.worldfloods.configs import CHANNELS_CONFIGURATIONS\n",
        "from ml4floods.data.worldfloods.dataset import WorldFloodsDatasetTiled\n",
        "# from ml4floods.data.worldfloods.lightning import WorldFloodsDataModule\n",
        "from ml4floods.models.utils.configuration import AttrDict\n",
        "import pytorch_lightning as pl\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Callable, Tuple, Optional\n",
        "from ml4floods.preprocess.worldfloods import prepare_patches\n",
        "from ml4floods.models.config_setup import load_json, get_filesystem\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def filenames_train_test_split(bucket_name:Optional[str], train_test_split_file:str) -> Dict[str, Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Read train test split json file from remote if needed.\n",
        "\n",
        "    Args:\n",
        "        bucket_name:\n",
        "        train_test_split_file:\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \"\"\"\n",
        "    if bucket_name or train_test_split_file.startswith(\"gs://\"):\n",
        "        if not train_test_split_file.startswith(\"gs://\"):\n",
        "            train_test_split_file = f\"gs://{bucket_name}/{train_test_split_file}\"\n",
        "        filenames_train_test = load_json(train_test_split_file)\n",
        "\n",
        "        # Preprend the bucket name to the files\n",
        "        for splitname, split in filenames_train_test.items():\n",
        "            for foldername, listoftiffs in split.items():\n",
        "                for idx,tiffname in enumerate(listoftiffs):\n",
        "                    if not tiffname.startswith(\"gs://\"):\n",
        "                        tiffname = f\"gs://{bucket_name}/{tiffname}\"\n",
        "                        # assert fs.exists(tiffname), f\"File {tiffname} does not exists\"\n",
        "                        listoftiffs[idx] = tiffname\n",
        "    else:\n",
        "        with open(train_test_split_file, \"r\") as fh:\n",
        "            filenames_train_test = json.load(fh)\n",
        "\n",
        "    return filenames_train_test\n",
        "\n",
        "\n",
        "def validate_worldfloods_data(path_to_splits:str,\n",
        "                              split_folders:List[str] = [\"train\", \"val\", \"test\"],\n",
        "                              folders_to_test:List[str] = [\"S2\", \"gt\"],\n",
        "                              verbose:bool=True):\n",
        "    \"\"\"\n",
        "    Check the structure of the data follows the expected convention\n",
        "    Args:\n",
        "        path_to_splits: path where train/test/val splits are.\n",
        "        split_folders: Split folders to check\n",
        "        folders_to_test: products to test (i.e. this could be S2, PERMANENTWATERJRC, floodmaps,...)\n",
        "        verbose: prints success\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError if something goes wrong\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_splits):\n",
        "        raise FileNotFoundError(f\" Path to splits folder not found {path_to_splits}\")\n",
        "\n",
        "    for isplit in split_folders:\n",
        "        folder_split = os.path.join(path_to_splits, isplit)\n",
        "        if not os.path.exists(folder_split):\n",
        "            raise FileNotFoundError(f\"Splits folder not found {folder_split}\")\n",
        "\n",
        "        filenames = None\n",
        "        for foldername in folders_to_test:\n",
        "            folder_path = os.path.join(folder_split, foldername)\n",
        "            if not os.path.exists(folder_path):\n",
        "                raise FileNotFoundError(f\" Folder not found {folder_path}\")\n",
        "            filenames_folder = [os.path.basename(os.path.splitext(f)[0]) for f in sorted(glob(os.path.join(folder_path, \"*\")))]\n",
        "            if len(filenames_folder) == 0:\n",
        "                raise FileNotFoundError(f\"Folder {folder_path} does not have any files on it\")\n",
        "            if filenames is None:\n",
        "                filenames = filenames_folder\n",
        "            else:\n",
        "                if filenames != filenames_folder:\n",
        "                    raise FileNotFoundError(f\"Different files {filenames} {filenames_folder}\")\n",
        "    if verbose:\n",
        "        print(\"Data downloaded follows the expected format\")\n",
        "\n",
        "\n",
        "def process_filename_train_test(train_test_split_file:Optional[str]=\"gs://ml4cc_data_lake/2_PROD/2_Mart/worldfloods_v1_0/train_test_split.json\",\n",
        "                                input_folder:str=\"S2\",\n",
        "                                target_folder:str=\"gt\",\n",
        "                                bucket_id:Optional[str]=None, path_to_splits:Optional[str]=None,\n",
        "                                download:Optional[Dict[str,bool]]=None) -> Dict[str,Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    The train_test_split_file contains which files go to the train/val/test splits. This function validate that\n",
        "    the content is as expected and that all files referred there exists. Additionally it downloads the data to loca\n",
        "    if specified.\n",
        "\n",
        "    Args:\n",
        "        train_test_split_file:\n",
        "        input_folder:\n",
        "        target_folder:\n",
        "        bucket_id:\n",
        "        path_to_splits:\n",
        "        download: e.g. `{\"train\": True, \"val\": False, \"test\": True}` to download train and val data.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if download is None:\n",
        "        download = {\"train\": False, \"val\": False, \"test\": False}\n",
        "\n",
        "    if train_test_split_file:\n",
        "        filenames_train_test = filenames_train_test_split(bucket_id, train_test_split_file)\n",
        "    else:\n",
        "        assert (path_to_splits is not None) and os.path.exists(path_to_splits), \\\n",
        "            f\"train_test_split_file not provided and path_to_splits folder {path_to_splits} does not exist\"\n",
        "\n",
        "        print(f\"train_test_split_file not provided. We will use the content in the folder {path_to_splits}\")\n",
        "        filenames_train_test = {'train': {target_folder:[], input_folder:[]},\n",
        "                                'test': {target_folder:[],input_folder:[]},\n",
        "                                'val': {target_folder:[],input_folder:[]}}\n",
        "\n",
        "    # loop through the naming splits\n",
        "    for isplit in [\"train\", \"test\", \"val\"]:\n",
        "        for foldername in [input_folder, target_folder]:\n",
        "\n",
        "            # glob files in path_to_splits dir if there're not files in the given split\n",
        "            if len(filenames_train_test[isplit][foldername]) == 0:\n",
        "                # get the subdirectory\n",
        "                assert (path_to_splits is not None) and os.path.exists(path_to_splits), \\\n",
        "                    f\"path_to_splits {path_to_splits} doesn't exists or not provided and there're no files in split {isplit} folder {foldername}\"\n",
        "\n",
        "                path_2_glob = os.path.join(path_to_splits, isplit, foldername, \"*.tif\")\n",
        "                filenames_train_test[isplit][foldername] = glob(path_2_glob)\n",
        "                assert len(filenames_train_test[isplit][foldername]) > 0, f\"No files found in {path_2_glob}\"\n",
        "\n",
        "        assert len(filenames_train_test[isplit][input_folder]) == len(filenames_train_test[isplit][target_folder]), \\\n",
        "            f\"Different number of files in {input_folder} and {target_folder} for split {isplit}: {len(filenames_train_test[isplit][input_folder])} {len(filenames_train_test[isplit][target_folder])}\"\n",
        "\n",
        "        # check correspondence input output files (assert files exists)\n",
        "        for idx, filename in enumerate(filenames_train_test[isplit][input_folder]):\n",
        "            fs = get_filesystem(filename)\n",
        "            assert fs.exists(filename), f\"File input: {filename} does not exists\"\n",
        "\n",
        "            filename_target = filenames_train_test[isplit][target_folder][idx]\n",
        "            assert fs.exists(filename_target), f\"File target: {filename_target} does not exists\"\n",
        "\n",
        "            # Download if needed and replace filenames_train_test with the downloaded version\n",
        "            if filename.startswith(\"gs://\") and download[isplit]:\n",
        "                assert (path_to_splits is not None) and os.path.exists(path_to_splits), \\\n",
        "                    f\"path_to_splits {path_to_splits} doesn't exists or not provided ad requested to download the data\"\n",
        "\n",
        "                for input_target_folder in [input_folder, target_folder]:\n",
        "                    folder_local = os.path.join(path_to_splits, isplit, input_target_folder)\n",
        "                    os.makedirs(folder_local, exist_ok=True)\n",
        "                    basename = os.path.basename(filename)\n",
        "                    file_src = filenames_train_test[isplit][input_target_folder][idx]\n",
        "                    file_dest = os.path.join(folder_local, basename)\n",
        "                    if not os.path.isfile(file_dest):\n",
        "                        fs.get_file(file_src, file_dest)\n",
        "                        print(f\"Downloaded ({idx}/{len(filenames_train_test[isplit][input_target_folder])}) {file_src}\")\n",
        "                    filenames_train_test[isplit][input_target_folder][idx] = file_dest\n",
        "\n",
        "    return filenames_train_test\n",
        "\n",
        "\n",
        "def get_dataset(data_config) -> pl.LightningDataModule:\n",
        "    \"\"\"\n",
        "    Function to set up dataloaders for model training\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Setup transformations for dataset\n",
        "\n",
        "    train_transform, test_transform = get_transformations(data_config)\n",
        "\n",
        "    # ======================================================\n",
        "    # Obtain train/val/test files\n",
        "    # ======================================================\n",
        "    download = data_config.get(\"download\")\n",
        "    if download is None:\n",
        "        download = {\"train\": data_config.get(\"loader_type\",\"local\")==\"local\",\n",
        "                    \"val\": data_config.get(\"loader_type\",\"local\")==\"local\",\n",
        "                    \"test\": data_config.get(\"loader_type\",\"local\")==\"local\"}\n",
        "\n",
        "    filenames_train_test = process_filename_train_test(data_config.get(\"train_test_split_file\"),\n",
        "                                                       input_folder=data_config.input_folder,\n",
        "                                                       target_folder=data_config.target_folder,\n",
        "                                                       bucket_id=data_config.get(\"bucket_id\"),\n",
        "                                                       path_to_splits=data_config.get(\"path_to_splits\"),\n",
        "                                                       download=download)\n",
        "\n",
        "    filter_windows_attr = data_config.get(\"filter_windows\", None)\n",
        "    if filter_windows_attr is not None and filter_windows_attr.get(\"apply\", False):\n",
        "        filter_windows_config = filter_windows_fun(data_config.filter_windows.version, data_config.train_test_split_file,\n",
        "                                                   threshold_clouds=data_config.filter_windows.threshold_clouds,\n",
        "                                                   local_destination_dir=data_config.path_to_splits)\n",
        "    else:\n",
        "        filter_windows_config = None\n",
        "\n",
        "    # CREATE DATAMODULE\n",
        "    datamodule = WorldFloodsDataModule(\n",
        "        filenames_train_test=filenames_train_test,\n",
        "        input_folder=data_config.input_folder,\n",
        "        target_folder=data_config.target_folder,\n",
        "        train_transformations=train_transform,\n",
        "        test_transformations=test_transform,\n",
        "        bands=CHANNELS_CONFIGURATIONS[data_config.channel_configuration],\n",
        "        add_mndwi_input = data_config.add_mndwi_input,\n",
        "        num_workers=data_config.num_workers,\n",
        "        window_size=data_config.window_size,\n",
        "        batch_size=data_config.batch_size,\n",
        "        filter_windows= filter_windows_config\n",
        "    )\n",
        "    datamodule.setup()\n",
        "\n",
        "    print(\"train\", datamodule.train_dataset.__len__(), \" tiles\")\n",
        "    print(\"val\", datamodule.val_dataset.__len__(), \" tiles\")\n",
        "    print(\"test\", datamodule.test_dataset.__len__(), \" tiles\")\n",
        "\n",
        "    return datamodule\n",
        "\n",
        "def filter_windows_fun(data_version:str, train_test_split_file:str, local_destination_dir:Optional[str]=None, threshold_clouds=.5) -> Callable:\n",
        "    \"\"\"\n",
        "    Returns a function to filter the windows in the  WorldFloodsDatasetTiled dataset. This is used for pre-filtering\n",
        "    the training images to discard patches with high cloud content.\n",
        "\n",
        "    Args:\n",
        "        data_version: \"v1\" or \"v2\"\n",
        "        local_destination_dir: local destination to save the json file with the windows to use\n",
        "        threshold_clouds: threshold to use to filter the window (window will be filtered if it has more than threshold_clouds\n",
        "        clouds)\n",
        "\n",
        "    Returns:\n",
        "        function to filter windows\n",
        "\n",
        "    \"\"\"\n",
        "    if local_destination_dir is not None:\n",
        "        if train_test_split_file:\n",
        "            split_name = \"_\"+os.path.basename(train_test_split_file)\n",
        "        else:\n",
        "            split_name =\".json\"\n",
        "        #windows_file = os.path.join(local_destination_dir, f\"windows_{data_version}.json\")\n",
        "        windows_file = os.path.join(local_destination_dir, f\"windows{split_name}\")\n",
        "    else:\n",
        "        windows_file = None\n",
        "\n",
        "    def filter_windows(tiledDataset: WorldFloodsDatasetTiled) -> List[WindowSlices]:\n",
        "        if windows_file and os.path.exists(windows_file):\n",
        "            selected_windows = load_windows(windows_file)\n",
        "        else:\n",
        "            if data_version == \"v1\":\n",
        "                selected_windows =  prepare_patches.filter_windows_v1(tiledDataset,\n",
        "                                                                      threshold_clouds=threshold_clouds)\n",
        "            elif data_version == \"v2\":\n",
        "                selected_windows = prepare_patches.filter_windows_v2(tiledDataset,\n",
        "                                                                     threshold_clouds=threshold_clouds)\n",
        "            else:\n",
        "                raise NotImplementedError(f\"Unknown ground truth version {data_version} expected v1 or v2\")\n",
        "\n",
        "            if windows_file:\n",
        "                save_windows(selected_windows, windows_file)\n",
        "\n",
        "        return selected_windows\n",
        "\n",
        "    return filter_windows\n",
        "\n",
        "\n",
        "def get_transformations(data_config) -> Tuple[Callable, Callable]:\n",
        "    \"\"\"\n",
        "    Function to generate transformations object to pass to dataloader\n",
        "    TODO: Build from config instead of using default values\n",
        "    \"\"\"\n",
        "\n",
        "    train_transform = [\n",
        "        transformations.InversePermuteChannels(),\n",
        "        transformations.RandomRotate90(always_apply=True, p=0.5),\n",
        "        transformations.Flip(always_apply=True, p=0.5)]\n",
        "\n",
        "    if \"train_transformation\" not in data_config:\n",
        "        warnings.warn(\"Train transformation not found in data config. Assume normalize is True\")\n",
        "        data_config[\"train_transformation\"] = AttrDict({\"normalize\": True})\n",
        "\n",
        "    channel_mean = None\n",
        "    if data_config.train_transformation.normalize:\n",
        "        channel_mean, channel_std = wf_normalization.get_normalisation(data_config.channel_configuration)\n",
        "        if data_config.add_mndwi_input:\n",
        "            channel_mean = np.concatenate([channel_mean,np.zeros((1,1,1))],axis = -1)\n",
        "            channel_std = np.concatenate([channel_std,np.ones((1,1,1))],axis = -1)\n",
        "\n",
        "        train_transform.append(transformations.Normalize(\n",
        "            mean=channel_mean,\n",
        "            std=channel_std,\n",
        "            max_pixel_value=1))\n",
        "\n",
        "    train_transform.extend([\n",
        "        transformations.PermuteChannels(),\n",
        "        transformations.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_transform = transformations.Compose(train_transform, is_check_shapes=False)\n",
        "\n",
        "    if \"test_transformation\" not in data_config:\n",
        "        warnings.warn(\"Test transformation not found in data config. Assume normalize is True\")\n",
        "        data_config[\"test_transformation\"] = AttrDict({\"normalize\": True})\n",
        "\n",
        "    if data_config.test_transformation.normalize:\n",
        "        if channel_mean is None:\n",
        "            channel_mean, channel_std = wf_normalization.get_normalisation(data_config.channel_configuration)\n",
        "\n",
        "        test_transform = [\n",
        "        transformations.InversePermuteChannels(),\n",
        "        transformations.Normalize(\n",
        "            mean=channel_mean,\n",
        "            std=channel_std,\n",
        "            max_pixel_value=1),\n",
        "        transformations.PermuteChannels(),\n",
        "        transformations.ToTensor(),\n",
        "        ]\n",
        "        test_transform = transformations.Compose(test_transform, is_check_shapes=False)\n",
        "    else:\n",
        "        test_transform = transformations.ToTensor()\n",
        "\n",
        "    return train_transform, test_transform"
      ]
    }
  ]
}